{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0045cd26",
   "metadata": {},
   "source": [
    "# EDA - ê²°ì¸¡ì¹˜/ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬(ì‹¬í™”)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdccd89",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# ì˜¨ë¼ì¸ ì†Œë§¤ ë°ì´í„° ìƒì„± (ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë²„ì „)\n",
    "np.random.seed(42)\n",
    "n_customers = 1000\n",
    "\n",
    "def create_realistic_ecommerce_data(n=1000):\n",
    "    \"\"\"ì‹¤ì œì™€ ìœ ì‚¬í•œ ì „ììƒê±°ë˜ ë°ì´í„° ìƒì„± (ì†Œë¬¸ì ì»¬ëŸ¼ëª…)\"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # ê³ ê° ê¸°ë³¸ ì •ë³´\n",
    "    data['customer_id'] = range(1, n+1)\n",
    "    data['age'] = np.random.normal(35, 12, n).clip(18, 80).astype(int)\n",
    "    data['gender'] = np.random.choice(['M', 'F'], n, p=[0.45, 0.55])\n",
    "    data['city'] = np.random.choice(['Seoul', 'Busan', 'Daegu', 'Incheon', 'Gwangju'], \n",
    "                                   n, p=[0.4, 0.2, 0.15, 0.15, 0.1])\n",
    "    \n",
    "    # êµ¬ë§¤ í–‰ë™ ë°ì´í„°\n",
    "    data['total_purchases'] = np.random.poisson(8, n) + 1\n",
    "    data['avg_order_value'] = np.random.lognormal(4.5, 0.8, n).round(2)\n",
    "    data['days_since_last_purchase'] = np.random.exponential(30, n).astype(int)\n",
    "    \n",
    "    # ë§Œì¡±ë„ ë° ì¶©ì„±ë„\n",
    "    data['satisfaction_score'] = np.random.normal(3.8, 1.2, n).clip(1, 5).round(1)\n",
    "    data['loyalty_points'] = (data['total_purchases'] * data['avg_order_value'] * 0.1 + \n",
    "                             np.random.normal(0, 100, n)).clip(0, None).round(0)\n",
    "    \n",
    "    # ì¹´í…Œê³ ë¦¬ë³„ êµ¬ë§¤ ê¸ˆì•¡\n",
    "    categories = ['electronics', 'clothing', 'books', 'home', 'sports']\n",
    "    for cat in categories:\n",
    "        # ì¼ë¶€ ê³ ê°ì€ íŠ¹ì • ì¹´í…Œê³ ë¦¬ì—ì„œ êµ¬ë§¤í•˜ì§€ ì•ŠìŒ\n",
    "        values = np.random.lognormal(3, 1, n)\n",
    "        # 30% í™•ë¥ ë¡œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ êµ¬ë§¤ ì•ˆ í•¨ (0ìœ¼ë¡œ ì„¤ì •)\n",
    "        mask = np.random.random(n) < 0.3\n",
    "        values[mask] = 0\n",
    "        data[f'{cat}_spending'] = values.round(2)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # ì˜ë„ì  ê²°ì¸¡ê°’ ìƒì„± (ì‹¤ì œ ìƒí™© ëª¨ë°©)\n",
    "    \n",
    "    # 1. MCAR(Missing Completely at Random): ì™„ì „ ë¬´ì‘ìœ„ ê²°ì¸¡ (ì‹œìŠ¤í…œ ì˜¤ë¥˜)\n",
    "    missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
    "    df.loc[missing_indices, 'satisfaction_score'] = np.nan\n",
    "    \n",
    "    # 2. MAR(Missing at Random): ì¡°ê±´ë¶€ ê²°ì¸¡ (ë‚˜ì´ê°€ ë†’ì„ìˆ˜ë¡ ë§Œì¡±ë„ ì‘ë‹µ ê±°ë¶€ìœ¨ ì¦ê°€)\n",
    "    elderly_mask = df['age'] > 50\n",
    "    elderly_missing = np.random.random(elderly_mask.sum()) < 0.15  \n",
    "    elderly_indices = df[elderly_mask].index[elderly_missing]\n",
    "    df.loc[elderly_indices, 'satisfaction_score'] = np.nan\n",
    "    \n",
    "    # 3. MNAR(Missing Not at Random): ë¹„ë¬´ì‘ìœ„ ê²°ì¸¡ - ê²°ì¸¡ ìì²´ê°€ ì˜ë¯¸ (ë†’ì€ ì†Œë“ìë“¤ì´ ê°œì¸ì •ë³´ ë¹„ê³µê°œ)\n",
    "    high_spenders = df['avg_order_value'] > df['avg_order_value'].quantile(0.8)\n",
    "    high_spender_missing = np.random.random(high_spenders.sum()) < 0.25\n",
    "    high_spender_indices = df[high_spenders].index[high_spender_missing]\n",
    "    df.loc[high_spender_indices, 'age'] = np.nan\n",
    "    \n",
    "    # ë„ì‹œ ì •ë³´ ì¼ë¶€ ê²°ì¸¡ (ë°°ì†¡ì§€ ë¯¸ì…ë ¥)\n",
    "    city_missing = np.random.choice(df.index, size=int(0.08 * len(df)), replace=False)\n",
    "    df.loc[city_missing, 'city'] = np.nan\n",
    "    \n",
    "    # ì˜ë„ì  ì´ìƒê°’ ìƒì„±\n",
    "    \n",
    "    # 1. ë°ì´í„° ì…ë ¥ ì˜¤ë¥˜ (ë‚˜ì´ 999ì‚´)\n",
    "    error_indices = np.random.choice(df.index, size=3, replace=False)\n",
    "    df.loc[error_indices, 'age'] = 999\n",
    "    \n",
    "    # 2. ë¹„ì¦ˆë‹ˆìŠ¤ ì´ìƒê°’ (VIP ê³ ê°ì˜ ê·¹ë„ë¡œ ë†’ì€ êµ¬ë§¤ì•¡) Extreme but Valid Outliers\n",
    "    vip_indices = np.random.choice(df.index, size=5, replace=False)\n",
    "    df.loc[vip_indices, 'avg_order_value'] *= 20\n",
    "    df.loc[vip_indices, 'loyalty_points'] *= 10\n",
    "    \n",
    "    # 3. ì‹œìŠ¤í…œ ë²„ê·¸ë¡œ ì¸í•œ ìŒìˆ˜ê°’\n",
    "    bug_indices = np.random.choice(df.index, size=2, replace=False) \n",
    "    df.loc[bug_indices, 'days_since_last_purchase'] = -1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ë°ì´í„° ìƒì„±\n",
    "ecommerce = create_realistic_ecommerce_data(1000)\n",
    "\n",
    "print(\"=== ì˜¨ë¼ì¸ ì†Œë§¤ ë°ì´í„° ê°œìš” ===\")\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {ecommerce.shape}\")\n",
    "print(\"\\në°ì´í„° ìƒ˜í”Œ:\")\n",
    "display(ecommerce.head(10))\n",
    "\n",
    "print(f\"\\nê¸°ë³¸ ì •ë³´:\")\n",
    "print(ecommerce.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f691ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ë°˜ì ì¸ ìƒíƒœ ì ê²€ - da_utils/profile.py ëª¨ë“ˆ(íŒŒì¼) ì•ˆì˜ get_data_profile í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜´\n",
    "from da_utils.profile import get_data_profile # ë°”ë¡œ get_data_profile í•¨ìˆ˜ë¥¼ êº¼ëƒˆìŒ\n",
    "# from da_utils import profile                # profile ëª¨ë“ˆ ì „ì²´ë¥¼ êº¼ë‚´ì„œ ì‹¤ì œë¡œ ì‚¬ìš©í•  ë•ŒëŠ” profile.get_data_profile ë¡œ ì‚¬ìš©í•´ì•¼ í•¨\n",
    "\n",
    "data_report = get_data_profile(ecommerce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f12a9a",
   "metadata": {},
   "source": [
    "## ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bfdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (ì„ íƒì‚¬í•­)\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def analyze_missing_patterns(df: pd.DataFrame):\n",
    "    \"\"\"ê²°ì¸¡ê°’ íŒ¨í„´ ì¢…í•© ë¶„ì„\"\"\"\n",
    "    \n",
    "    print('=== ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„ ===')\n",
    "    missing_info = df.isna().sum()\n",
    "    missing_pct = (missing_info / len(df)) * 100\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'ê²°ì¸¡ìˆ˜': missing_info,\n",
    "        'ê²°ì¸¡ë¥ (%)': missing_pct.round(2),\n",
    "    })\n",
    "    missing_summary = missing_summary[missing_summary['ê²°ì¸¡ìˆ˜'] > 0].sort_values('ê²°ì¸¡ìˆ˜', ascending=False)\n",
    "    print('ë³€ìˆ˜ë³„ ê²°ì¸¡ í˜„í™©')\n",
    "    display(missing_summary)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15,10))\n",
    "    a1, a2, a3, a4 = axes[0,0], axes[0,1], axes[1,0], axes[1,1]\n",
    "    \n",
    "    # 1. ê²°ì¸¡ê°’ íˆíŠ¸ë§µ\n",
    "    sns.heatmap(df.isna(), yticklabels=False, cbar=True, cmap='viridis', ax=a1)\n",
    "    a1.set_title('ê²°ì¸¡ê°’ íŒ¨í„´ íˆíŠ¸ë§µ')\n",
    "    \n",
    "    # 2. ë³€ìˆ˜ë³„ ê²°ì¸¡ë¥  ë°” ì°¨íŠ¸\n",
    "    if len(missing_summary) > 0:  # >0ì€ ìƒëµ ê°€ëŠ¥(ë‚´ìš©ì´ ìˆë‹¤ë©´)\n",
    "        missing_summary['ê²°ì¸¡ë¥ (%)'].plot(kind='bar', color='coral', ax=a2)\n",
    "        a2.set_title('ë³€ìˆ˜ë³„ ê²°ì¸¡ë¥ ')    \n",
    "        a2.set_ylabel('ê²°ì¸¡ë¥ (%)')\n",
    "        a2.tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # 3. ê²°ì¸¡ê°’ ì¡°í•© íŒ¨í„´ - ê²°ì¸¡ì´ ë™ì‹œì— ë°œìƒí•˜ëŠ” ë³€ìˆ˜ íŒŒì•… í›„ ìœ ì˜ë¯¸í•˜ê²Œ ë§ìœ¼ë©´ ì—°ê´€ì„ ìƒê°í•  ìˆ˜ ìˆìŒ -> 4ë²ˆì—ì„œ ê·¸ë£¹ë³„ë¡œ ì²˜ë¦¬ ê°€ëŠ¥\n",
    "    '''\n",
    "    df.isna().any()                           -> ì»¬ëŸ¼ì— ê²°ì¸¡ê°’ì´ ìˆìœ¼ë©´ True, ì—†ìœ¼ë©´ False\n",
    "    df.columns[df.isna().any()]               -> ê²°ì¸¡ê°’ì´ ìˆëŠ” ì»¬ëŸ¼ëª…ì„ ì¶”ì¶œ\n",
    "    df[df.columns[df.isna().any()]]           -> ê²°ì¸¡ê°’ì´ ìˆëŠ” ì»¬ëŸ¼ì˜ ì „ì²´ê°’ì„ ì¶”ì¶œí•¨\n",
    "    df[df.columns[df.isna().any()]].isna()    -> ê²°ì¸¡ê°’ì´ë¼ë©´ Trueë¡œ ì„¤ì •í•¨\n",
    "    ê²°ì¸¡ê°’ì´ ìˆëŠ” age, city, satisfaction_score ê°„ì— ì–´ë–¤ ì¡°í•©ì´ ê°€ì¥ ë§ì€ì§€ íŒŒì•…\n",
    "    '''\n",
    "    missing_pattern = df[df.columns[df.isna().any()]].isna()\n",
    "    if len(missing_pattern):\n",
    "        pattern_counts = missing_pattern.value_counts().head(10)\n",
    "        pattern_counts.plot(kind='bar', color='lightblue', ax=a3)\n",
    "        a3.set_title('ê²°ì¸¡ íŒ¨í„´ ì¡°í•©(ìƒìœ„ 10ê°œ)')\n",
    "        a3.set_ylabel('ë¹ˆë„')\n",
    "        a3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "\n",
    "    # 4. ê²°ì¸¡ ë³€ìˆ˜ë³„ ê²°ì¸¡ ì—¬ë¶€(0/1)ì™€ ë‹¤ë¥¸ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ì‹œê°í™”\n",
    "    numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "    missing_cols = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "    if len(numeric_cols) > 0 and len(missing_cols) > 0:\n",
    "        # ê²°ì¸¡ê°’ì„ 0/1ë¡œ ë³€í™˜í•œ DataFrame ìƒì„±\n",
    "        missing_binary = df[missing_cols].isna().astype(int)\n",
    "        missing_binary.columns = [f'{col}_missing' for col in missing_binary.columns]\n",
    "        \n",
    "        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì™€ ê²°ì¸¡ íŒ¨í„´ ë³€ìˆ˜ ê²°í•©\n",
    "        corr_data = pd.concat([df[numeric_cols], missing_binary], axis=1)\n",
    "        \n",
    "        # ìƒê´€ê³„ìˆ˜ ê³„ì‚°\n",
    "        correlation_matrix = corr_data.corr()\n",
    "        \n",
    "        # ê²°ì¸¡ íŒ¨í„´ ë³€ìˆ˜ì™€ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë§Œ ì¶”ì¶œ\n",
    "        missing_numeric_corr = correlation_matrix.loc[\n",
    "            missing_binary.columns, \n",
    "            numeric_cols\n",
    "        ]\n",
    "        \n",
    "        # ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°\n",
    "        if missing_numeric_corr.shape[0] > 0 and missing_numeric_corr.shape[1] > 0:\n",
    "            sns.heatmap(\n",
    "                    missing_numeric_corr, \n",
    "                    annot=True, \n",
    "                    cmap='coolwarm', \n",
    "                    center=0,\n",
    "                    fmt='.2f',\n",
    "                    ax=a4,\n",
    "                    cbar_kws={'label': 'ìƒê´€ê³„ìˆ˜'},\n",
    "                )\n",
    "            a4.set_title('ê²°ì¸¡ íŒ¨í„´ê³¼ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„')\n",
    "            a4.set_xlabel('ìˆ˜ì¹˜í˜• ë³€ìˆ˜')\n",
    "            a4.set_ylabel('ê²°ì¸¡ íŒ¨í„´')\n",
    "        else:\n",
    "            a4.text(0.5, 0.5, 'ë¶„ì„í•  ìƒê´€ê´€ê³„ ì—†ìŒ', ha='center', va='center')\n",
    "            a4.axis('off')\n",
    "    else:\n",
    "        a4.text(0.5, 0.5, 'ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë˜ëŠ”\\nê²°ì¸¡ê°’ì´ ì—†ìŒ', ha='center', va='center')\n",
    "        a4.axis('off')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "analyze_missing_patterns(ecommerce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b022406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì´ì¬ ëª¨ë“ˆì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "from da_utils.patterns import analyze_missing_patterns\n",
    "analyze_missing_patterns(ecommerce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e098e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ ë©”ì»¤ë‹ˆì¦˜ ì§„ë‹¨\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "\n",
    "print('=== ê²°ì¸¡ ë©”ì»¤ë‹ˆì¦˜ ì§„ë‹¨ ===')\n",
    "\n",
    "# MAR(ì¡°ê±´ë¶€) íŒ¨í„´\n",
    "# ê°€ì„¤: ë‚˜ì´ ê²°ì¸¡ê³¼ êµ¬ë§¤ì•¡ ê°„ ê´€ê³„\n",
    "if ecommerce['age'].isna().sum():\n",
    "    print('\\n1. ë‚˜ì´(age) ê²°ì¸¡ ë©”ì»¤ë‹ˆì¦˜ ë¶„ì„')\n",
    "\n",
    "    # êµ¬ë§¤ì•¡ ë¶„ìœ„ë³„ ë‚˜ì´ ê²°ì¸¡ë¥ \n",
    "    df_temp = ecommerce.copy()\n",
    "    df_temp['spending_quertile'] = pd.qcut(df_temp['avg_order_value'].dropna(), q=4, labels=['4ë¶„ìœ„', '3ë¶„ìœ„', '2ë¶„ìœ„', '1ë¶„ìœ„'])\n",
    "    age_missing_by_spending = df_temp.groupby('spending_quertile')['age'].apply(lambda x: x.isnull().mean())  # ë¶„ìœ„ë³„ë¡œ ê·¸ë£¹ì„ ì§“ê³ , ê·¸ë£¹ë³„ë¡œ ê° ê°’ì— isnull(T/Fë¡œ ë³€í™˜)ì„ ì ìš© í›„ í‰ê· (T/FëŠ” 1/0)ì„ ê³„ì‚°\n",
    "    print('\\n ğŸ’²êµ¬ë§¤ì•¡ ë¶„ìœ„ë³„ ë‚˜ì´ ê²°ì¸¡ë¥ ')\n",
    "    for key, value in age_missing_by_spending.items():\n",
    "        print(f' {key}: {value:.1%}')\n",
    "        \n",
    "    con_table = pd.crosstab(df_temp['spending_quertile'], df_temp['age'].isnull())\n",
    "    chi2, p_value, _, _ = chi2_contingency(con_table)\n",
    "\n",
    "    # print(f'\\nì¹´ì´^2 í†µê³„ëŸ‰: {chi2:.3f}, p-value: {p_value:.4f}')\n",
    "    if p_value < 0.05:\n",
    "        print(' âœ…MAR í™•ì¸: êµ¬ë§¤ì•¡ì— ë”°ë¼ ë‚˜ì´ ê²°ì¸¡ë¥ ì´ ìœ ì˜ë¯¸í•˜ê²Œ ë‹¤ë¦„')\n",
    "    else:\n",
    "        print(' âŒMCAR ê°€ëŠ¥ì„± ìˆìŒ: êµ¬ë§¤ì•¡ê³¼ ë‚˜ì´ ê²°ì¸¡ë¥ ì€ ì„œë¡œ ë…ë¦½ì ')\n",
    "\n",
    "else:\n",
    "    print(' ë‚˜ì´(age) ê²°ì¸¡ ì—†ìŒ')\n",
    "    \n",
    "    \n",
    "# ë§Œì¡±ë„ ê²°ì¸¡ íŒ¨í„´ ë¶„ì„\n",
    "print('\\n2. ë§Œì¡±ë„(satisfaction_score) ê²°ì¸¡ ë©”ì»¤ë‹ˆì¦˜ ë¶„ì„')\n",
    "\n",
    "if ecommerce['satisfaction_score'].isna().sum():\n",
    "    # ì—°ë ¹ëŒ€ë³„ ë§Œì¡±ë„ ê²°ì¸¡ë¥  [0, 30, 50, 100] - [ì²­ë…„ì¸µ(~30), ì¤‘ë…„ì¸µ(31~50), ë…¸ë…„ì¸µ(51~)]\n",
    "    # ê° êµ¬ê°„ë§ˆë‹¤ ë§Œì¡±ë„ê°€ ì—†ëŠ” ì‚¬ëŒë“¤ì˜ %ë¥¼ êµ¬í•´ì„œ print\n",
    "    print('\\n ğŸ§“ì—°ë ¹ë³„ ë§Œì¡±ë„ ê²°ì¸¡ë¥ ')\n",
    "    df_temp = ecommerce.copy()\n",
    "    df_temp['age_group'] = pd.cut(df_temp['age'].dropna(), bins=[0, 30, 50, 200], labels=['ì²­ë…„ì¸µ', 'ì¤‘ë…„ì¸µ', 'ë…¸ë…„ì¸µ'])\n",
    "    satisfaction_missing_by_age = df_temp.groupby('age_group')['satisfaction_score'].apply(lambda x: x.isnull().sum() / len(x))\n",
    "\n",
    "    for key, value in satisfaction_missing_by_age.items():\n",
    "        print(f' {key}: {value:.1%}')\n",
    "\n",
    "    con_table = pd.crosstab(df_temp['age_group'], df_temp['satisfaction_score'].isnull())\n",
    "    chi2, p_value, _, _ = chi2_contingency(con_table)\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(' âœ…MAR í™•ì¸: ì—°ë ¹ì— ë”°ë¼ ë§Œì¡±ë„ ê²°ì¸¡ë¥ ì´ ìœ ì˜ë¯¸í•˜ê²Œ ë‹¤ë¦„')\n",
    "    else:\n",
    "        print(' âŒMCAR ê°€ëŠ¥ì„± ìˆìŒ: ì—°ë ¹ê³¼ ë§Œì¡±ë„ ê²°ì¸¡ë¥ ì€ ì„œë¡œ ë…ë¦½ì ')\n",
    "\n",
    "\n",
    "    # ì„±ë³„ë³„ ë§Œì¡±ë„ ê²°ì¸¡ë¥ ë„\n",
    "    print('\\n â™‚ï¸ì„±ë³„ë³„ ë§Œì¡±ë„ ê²°ì¸¡ë¥ ')\n",
    "\n",
    "    df_temp = ecommerce.copy()\n",
    "    satisfaction_missing_by_gender = df_temp.groupby('gender')['satisfaction_score'].apply(lambda x: x.isnull().sum() / len(x))\n",
    "\n",
    "    for key, value in satisfaction_missing_by_gender.items():\n",
    "        print(f' {key}: {value:.1%}')\n",
    "\n",
    "\n",
    "    con_table = pd.crosstab(df_temp['gender'], df_temp['satisfaction_score'].isnull())\n",
    "    chi2, p_value, _, _ = chi2_contingency(con_table)\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(' âœ…MAR í™•ì¸: ì„±ë³„ì— ë”°ë¼ ë§Œì¡±ë„ ê²°ì¸¡ë¥ ì´ ìœ ì˜ë¯¸í•˜ê²Œ ë‹¤ë¦„')\n",
    "    else:\n",
    "        print(' âŒMCAR ê°€ëŠ¥ì„± ìˆìŒ: ì„±ë³„ê³¼ ë§Œì¡±ë„ ê²°ì¸¡ë¥ ì€ ì„œë¡œ ë…ë¦½ì ')\n",
    "        \n",
    "else:\n",
    "    print(' ë§Œì¡±ë„ ê²°ì¸¡ ì—†ìŒ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5294702",
   "metadata": {},
   "source": [
    "## ê³ ê¸‰ ê²°ì¸¡ê°’ ëŒ€ì²´ ê¸°ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute: ëŒ€ì²´í•˜ë‹¤\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ë½‘ê¸°\n",
    "numeric_cols = ['age', 'total_purchases', 'avg_order_value', 'days_since_last_purchase', 'satisfaction_score']\n",
    "df_numeric = ecommerce[numeric_cols]\n",
    "print('ì›ë³¸ë°ì´í„° ê²°ì¸¡ë¥ ')\n",
    "for col in numeric_cols:\n",
    "    missing_rate = df_numeric[col].isnull().mean()\n",
    "    if missing_rate:\n",
    "        print(f'  {col}: {missing_rate:.2%}')\n",
    "\n",
    "# 1. ê²°ì¸¡ì— ì „ë¶€ í‰ê· ì„ ì§‘ì–´ë„£ì—ˆìŒ\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "df_mean = df_numeric.copy()\n",
    "df_mean[numeric_cols] = imputer_mean.fit_transform(df_numeric[numeric_cols])\n",
    "\n",
    "\n",
    "# 2. KNN ëŒ€ì²´ - K Nearest Neighbors, ê²°ì¸¡ê°’ê³¼ ê°€ì¥ ê°€ê¹Œìš´ nëª…ì„ ë½‘ì•„ì„œ ê·¸ ì‚¬ëŒë“¤ì˜ í‰ê· ì„ êµ¬í•¨\n",
    "imputer_knn = KNNImputer(n_neighbors=5)\n",
    "df_knn = df_numeric.copy()\n",
    "df_knn[numeric_cols] = imputer_knn.fit_transform(df_numeric[numeric_cols])\n",
    "\n",
    "\n",
    "# 3. MICE ëŒ€ì²´ - Multiple Imputation by Chained Equations, ê²°ì¸¡ë³€ìˆ˜ë¥¼ ë‹¤ë¥¸ ë³€ìˆ˜ë¥¼ ì¨ì„œ íšŒê·€ëª¨ë¸ ì˜ˆì¸¡\n",
    "imputer_mice = IterativeImputer(random_state=42, max_iter=10)\n",
    "df_mice = df_numeric.copy()\n",
    "df_mice[numeric_cols] = imputer_mice.fit_transform(df_numeric[numeric_cols])\n",
    "\n",
    "# age ë³€ìˆ˜ì˜ ëŒ€ì²´ê²°ê³¼ ë¹„êµ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15,10))\n",
    "a1, a2, a3, a4 = axes[0,0], axes[0,1], axes[1,0], axes[1,1]\n",
    "\n",
    "variable = 'age'\n",
    "if df_numeric[variable].isnull().sum():\n",
    "    \n",
    "    sns.histplot(df_mean[variable], binwidth=2, kde=False, ax=a1, label='í‰ê· ', color='blue')\n",
    "    sns.histplot(df_numeric[variable], binwidth= 2, kde=False, ax=a1, label='ì›ë³¸', color='lightblue', alpha=0.3)\n",
    "    a1.set_xlim(0, 100)\n",
    "    a1.set_title('í‰ê· ')\n",
    "        \n",
    "    sns.histplot(df_knn[variable], binwidth= 2, kde=False, ax=a2, label = 'KNN', color='coral')\n",
    "    sns.histplot(df_numeric[variable], binwidth= 2, kde=False, ax=a2, label='ì›ë³¸', color='lightcoral', alpha=0.3)\n",
    "    a2.set_xlim(0, 100)\n",
    "    a2.set_title('KNN')\n",
    "\n",
    "    sns.histplot(df_mice[variable], binwidth= 2, kde=False, ax=a3, label = 'mice', color='green')\n",
    "    sns.histplot(df_numeric[variable], binwidth= 2, kde=False, ax=a3, label='ì›ë³¸', color='lightgreen', alpha=0.3)\n",
    "    a3.set_xlim(0, 100)\n",
    "    a3.set_title('mice')\n",
    "    \n",
    "    \n",
    "    # ëŒ€ì²´ ì „í›„ í†µê³„ëŸ‰ ë¹„êµ\n",
    "    comparison_stats = pd.DataFrame({\n",
    "        'ì›ë³¸': df_numeric[variable].describe(),\n",
    "        'í‰ê· ': df_mean[variable].describe(),\n",
    "        'KNN': df_knn[variable].describe(),\n",
    "        'mice': df_mice[variable].describe(),\n",
    "    }).round(2)\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ë¡œ í†µê³„ ë¹„êµ í‘œì‹œ\n",
    "    a4.axis('off')\n",
    "    a4.table(\n",
    "        cellText=comparison_stats.values,\n",
    "        rowLabels=comparison_stats.index,\n",
    "        colLabels=comparison_stats.columns,\n",
    "        cellLoc='center',\n",
    "        loc='center',\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fe331",
   "metadata": {},
   "source": [
    "## ëŒ€ì²´ í’ˆì§ˆ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c895e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== ëŒ€ì²´ í’ˆì§ˆ í‰ê°€ ===')\n",
    "dist_evaluation_results = []\n",
    "corr_evaluation_results = []\n",
    "original_df = df_numeric\n",
    "imputed_dfs = [df_mean, df_knn, df_mice ]\n",
    "method_names = ['í‰ê· ëŒ€ì²´', 'KNNëŒ€ì²´', 'MICEëŒ€ì²´']\n",
    "\n",
    "for method_name, imputed_df in zip(method_names, imputed_dfs):\n",
    "    \n",
    "    for col in ['age','satisfaction_score']:\n",
    "        # 1. ë¶„í¬ ìœ ì‚¬ì„± í‰ê°€\n",
    "        # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ê²½ìš°ë§Œ\n",
    "        if original_df[col].isnull().sum():\n",
    "            \n",
    "            original_stats = original_df[col].dropna().describe()\n",
    "            imputed_stats = imputed_df[col].describe()\n",
    "            \n",
    "            # í‰ê· ì°¨ì´\n",
    "            mean_diff = abs(original_stats['mean'] - imputed_stats['mean']) / original_stats['mean'] * 100\n",
    "            # í‘œì¤€í¸ì°¨ì°¨ì´\n",
    "            std_diff = abs(original_stats['std'] - imputed_stats['std']) / original_stats['std'] * 100\n",
    "            \n",
    "            dist_evaluation_results.append({\n",
    "                'ë°©ë²•': method_name,\n",
    "                'ë³€ìˆ˜': col,\n",
    "                'í‰ê· ì°¨ì´(%)': mean_diff,\n",
    "                'í‘œì¤€í¸ì°¨ì°¨ì´(%)': std_diff,\n",
    "            })\n",
    "\n",
    "        # 2. ë‹¤ë¥¸ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„ ë³´ì¡´ í‰ê°€ - age, satisfaction_score <-> average_order_value\n",
    "        # ì´ê±´ ì‹¤ì œë¡œ ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ê±¸ë¡œ í‰ê°€í•´ì•¼ í•¨\n",
    "            original_corr = original_df[[col, 'avg_order_value']].corr().iloc[0, 1]  # ê²°ì¸¡ì¹˜ í¬í•¨\n",
    "            imputed_corr = imputed_df[[col, 'avg_order_value']].dropna().corr().iloc[0, 1]  # ê²°ì¸¡ì¹˜ ëŒ€ì²´\n",
    "        \n",
    "            # ìƒê´€ê´€ê³„ ìœ ì§€ë„\n",
    "            corr_preservation = abs(original_corr - imputed_corr) / abs(original_corr) * 100\n",
    "            \n",
    "            corr_evaluation_results.append({\n",
    "                'ë°©ë²•': method_name,\n",
    "                'ë³€ìˆ˜': f'{col}-êµ¬ë§¤ì•¡ ìƒê´€ê´€ê³„',\n",
    "                'ì›ë³¸ìƒê´€ê³„ìˆ˜': original_corr,\n",
    "                'ëŒ€ì²´ìƒê´€ê³„ìˆ˜': imputed_corr,\n",
    "                'ìƒê´€ê³„ìˆ˜ë³´ì¡´ë„(%)': 100 - corr_preservation,\n",
    "            })\n",
    "        \n",
    "print('\\n1. ë¶„í¬ ë³´ì¡´ ì„±ëŠ¥')\n",
    "dist_eval = pd.DataFrame(dist_evaluation_results) \n",
    "display(dist_eval)\n",
    "\n",
    "print('\\n2. ìƒê´€ê´€ê³„ ë³´ì¡´ ì„±ëŠ¥')\n",
    "corr_eval = pd.DataFrame(corr_evaluation_results)\n",
    "for _, row in corr_eval.iterrows():\n",
    "    print(f'{row['ë°©ë²•']}: ë³´ì¡´ë„ {row['ìƒê´€ê³„ìˆ˜ë³´ì¡´ë„(%)']:.1f}%')\n",
    "    print(f'(ì›ë³¸: {row['ì›ë³¸ìƒê´€ê³„ìˆ˜']:.3f} â†’ ëŒ€ì²´í›„: {row['ëŒ€ì²´ìƒê´€ê³„ìˆ˜']:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0285f",
   "metadata": {},
   "source": [
    "## ì´ìƒê°’ íƒì§€ ë° ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c02d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì¢…í•© ì´ìƒê°’ íƒì§€ ===\n",
      "\n",
      "1. ì¼ë³€ëŸ‰ ì´ìƒê°’ íƒì§€(IQRë²•)\n",
      "  age: 12ê°œ (1.2%)\n",
      "  avg_order_value: 70ê°œ (7.0%)\n",
      "  days_since_last_purchase: 48ê°œ (4.8%)\n",
      "\n",
      "2. ë‹¤ë³€ëŸ‰ ì´ìƒê°’ íƒì§€(ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬)\n",
      "  ì„ê³„ê°’: 4.53\n",
      "  ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬ ì´ìƒê°’: 11ê°œ (1.1%)\n",
      "\n",
      "3. ë‹¤ë³€ëŸ‰ ì´ìƒê°’ íƒì§€(Isolation Forest)\n",
      "  Isolation Forest ì´ìƒê°’: 127ê°œ (12.7%)\n",
      "\n",
      "4. ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê¸°ë°˜ ì´ìƒê°’ íƒì§€\n",
      "  ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì´ìƒê°’: 7ê°œ (0.7%)\n",
      "\n",
      " == ìµœì¢… ì´ìƒê°’: 73ê°œ (7.3%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IQR</th>\n",
       "      <th>ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬</th>\n",
       "      <th>Isolation Forest</th>\n",
       "      <th>ë¹„ì¦ˆë‹ˆìŠ¤</th>\n",
       "      <th>ì´ì´ìƒê°’ìˆ˜</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       IQR  ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬  Isolation Forest   ë¹„ì¦ˆë‹ˆìŠ¤  ì´ì´ìƒê°’ìˆ˜\n",
       "0    False      False             False  False      0\n",
       "1    False      False             False  False      0\n",
       "2    False      False             False  False      0\n",
       "3    False      False             False  False      0\n",
       "4    False      False             False  False      0\n",
       "..     ...        ...               ...    ...    ...\n",
       "995   True      False              True  False      2\n",
       "996  False      False              True  False      1\n",
       "997  False      False              True  False      1\n",
       "998  False      False             False  False      0\n",
       "999  False      False             False  False      0\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# ê²½ê³ ë©”ì„¸ì§€ ë¬´ì‹œ\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "# ë²”ìš© ì´ìƒì¹˜ íƒì§€ í•¨ìˆ˜\n",
    "def outlier_detection(df: pd.DataFrame, chi_q=0.999, iso_contamination=0.1, final_threshold=2):\n",
    "    print('=== ì¢…í•© ì´ìƒê°’ íƒì§€ ===')\n",
    "    df_copy = df.copy()\n",
    "    numeric_data = df_copy.select_dtypes(include=['number'])\n",
    "    \n",
    "    # 1. IQR ì´ìƒê°’(ì¼(ë‹¨)ë³€ëŸ‰ ì´ìƒê°’ íƒì§€) - ë³€ìˆ˜ í•˜ë‚˜ë¥¼ ê°€ì§€ê³  í™•ì¸\n",
    "        # íŠ¹ì • ì»¬ëŸ¼ ì•ˆì—ì„œ ë‹¤ë¥¸ ê°’ë“¤ì— ë¹„í•´ ê°’ì´ ì´ìƒí•œ ê±¸ íƒì§€í•¨\n",
    "    print('\\n1. ì¼ë³€ëŸ‰ ì´ìƒê°’ íƒì§€(IQRë²•)')\n",
    "    univariate_outliers = pd.DataFrame(index=df_copy.index)\n",
    "    \n",
    "    for col in numeric_data.columns:\n",
    "        Q1 = df_copy[col].quantile(0.25)\n",
    "        Q3 = df_copy[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers_mask = (df_copy[col] < lower_bound) | (df_copy[col] > upper_bound)\n",
    "        univariate_outliers[col] = outliers_mask\n",
    "        \n",
    "        outlier_count = outliers_mask.sum()\n",
    "        if outlier_count:\n",
    "            print(f'  {col}: {outlier_count}ê°œ ({outlier_count/len(df_copy) * 100:.1f}%)')\n",
    "\n",
    "    # 2. ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬ ê¸°ë°˜ ë‹¤ë³€ëŸ‰ ì´ìƒê°’\n",
    "        # ë³€ìˆ˜ ê°„ì˜ ìŠ¤ì¼€ì¼ ì°¨ì´ì™€ ìƒê´€ê´€ê³„ê¹Œì§€ ê³ ë ¤í•´ ì´ìƒê°’ì„ í™•ì¸\n",
    "        # íŠ¹ì • ì»¬ëŸ¼ì´ ì´ìƒí•œê²Œ ì•„ë‹ˆë¼, ì¢…í•©ì ìœ¼ë¡œ ë°ì´í„° í–‰ í•˜ë‚˜í•˜ë‚˜ê°€ ì´ìƒí•œê°€ë¥¼ ë³´ì—¬ì¤€ë‹¤\n",
    "        # ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ ìœ ìš©í•¨(ë°ì´í„°ê°€ ëŒ€ë¶€ë¶„ ì •ìƒìœ¼ë¡œ ë³´ì´ì§€ë§Œ, ì´ìƒê°’ì„ ì°¾ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©) - ëŒ€ë†“ê³  ì´ìƒí•œ ë°ì´í„°ê°€ í•œê°€ë“ì¼ ë•ŒëŠ” ì œëŒ€ë¡œ ì‘ë™ ì•ˆí•¨. \n",
    "    print('\\n2. ë‹¤ë³€ëŸ‰ ì´ìƒê°’ íƒì§€(ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬)')\n",
    "    \n",
    "    # ë³€ìˆ˜ ê°„ ìŠ¤ì¼€ì¼ ì°¨ì´ ë³€í™˜\n",
    "        # standard scaler - ëª¨ë“  ë°ì´í„°ë¥¼ í‘œì¤€í¸ì°¨ 1, í‰ê·  0ìœ¼ë¡œ ë°”ê¿”ë²„ë¦¼\n",
    "    scaler = StandardScaler()\n",
    "    scaled_df = pd.DataFrame(\n",
    "        scaler.fit_transform(numeric_data),\n",
    "        columns=numeric_data.columns,\n",
    "        index=numeric_data.index,\n",
    "    )\n",
    "    # ë°ì´í„° í‰ê·  ë²¡í„°\n",
    "    mean = scaled_df.mean().values\n",
    "    # ê³µë¶„ì‚° í–‰ë ¬\n",
    "    cov_matrix = np.cov(scaled_df, rowvar=False)\n",
    "    # ê³µë¶„ì‚° í–‰ë ¬ì˜ ì—­í–‰ë ¬\n",
    "    inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "    \n",
    "    # ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬ ê³„ì‚°\n",
    "    mahalanobis_dist = scaled_df.apply(lambda row: distance.mahalanobis(row, mean, inv_cov_matrix), axis=1)\n",
    "    \n",
    "    # ì´ìƒì¹˜ ê¸°ì¤€ì (threshold) ì§€ì • (ì¹´ì´ì œê³± ë¶„í¬ -> ì •ìƒê°’ì„ ëª‡ í¼ì„¼íŠ¸(95% / 99% / 99.9%)ê¹Œì§€ ì¸ì •í•  ìˆ˜ ìˆëŠ”ê°€)\n",
    "    threshold = chi2.ppf(chi_q, len(numeric_data.columns)) ** 0.5\n",
    "    mahalanobis_outliers = mahalanobis_dist > threshold\n",
    "    print(f'  ì„ê³„ê°’: {threshold:.2f}')\n",
    "    print(f'  ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬ ì´ìƒê°’: {mahalanobis_outliers.sum()}ê°œ ({mahalanobis_outliers.mean() * 100:.1f}%)')\n",
    "    \n",
    "    # 3. Isolation Forest ê¸°ë°˜ ë‹¤ë³€ëŸ‰ ì´ìƒê°’\n",
    "        # ëœë¤í•œ ê¸°ì¤€ìœ¼ë¡œ êµ¬ë¶„í•˜ëŠ” ë…ë¦½ëœ Tree ì—¬ëŸ¬ê°œë¥¼ ë§Œë“¤ì–´ì„œ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ê³ ë¦½ë˜ëŠ”ì§€ë¥¼ í™•ì¸í•˜ëŠ” ê³¼ì •\n",
    "        # ë°ì´í„° ê°„ì˜ ê´€ê³„ê°€ ë³µì¡í•´ ì´ìƒì¹˜ê°€ ë³µì¡í•˜ê²Œ ìˆ¨ì–´ìˆì„ ë•Œ ì´ìš©í•¨ - ë§¤ìš° ê³µê²©ì ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ ê²€ì¶œ, ëŒ€ë†“ê³  ìˆ˜ìƒí•œ ë°ì´í„°ë“¤ì„ ì˜ ê³¨ë¼ì¤€ë‹¤\n",
    "    print('\\n3. ë‹¤ë³€ëŸ‰ ì´ìƒê°’ íƒì§€(Isolation Forest)')\n",
    "    # contamination = ì „ì²´ì—ì„œ ì–¼ë§ˆë‚˜ ì´ìƒê°’ ë¹„ìœ¨ì´ ìˆì„ê²ƒì¸ê°€ ì˜ˆì¸¡. 'auto'ë¡œ ì•Œì•„ì„œ ê³ ë¥´ë¼ í•  ìˆ˜ ìˆë‹¤.\n",
    "    iso_forest = IsolationForest(contamination=iso_contamination, random_state=42)\n",
    "    iso_outliers = iso_forest.fit_predict(scaled_df) == -1\n",
    "    iso_scores = iso_forest.score_samples(scaled_df)\n",
    "    print(f'  Isolation Forest ì´ìƒê°’: {iso_outliers.sum()}ê°œ ({iso_outliers.mean() * 100:.1f}%)')    \n",
    "    \n",
    "    # 4. ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™(íŠ¹í™”) ì´ìƒê°’\n",
    "        # êµ³ì´ ë³µì¡í•œ ë°©ë²•ì„ ì“°ì§€ ë§ê³ , ê° ì»¬ëŸ¼ë³„ë¡œ ì´ìƒê°’ ê¸°ì¤€ì„ ì •í•´ì¤€ ë’¤ ê·¸ ê¸°ì¤€ì— ë§ì§€ ì•Šìœ¼ë©´ ì´ìƒê°’ìœ¼ë¡œ íŒì •(ì‚¬ëŒì€ 130ì‚´ ì´ìƒ ì‚´ê¸° í˜ë“¦)\n",
    "        # ê° ë°ì´í„°ë§ˆë‹¤ ì „ìš© ê¸°ì¤€ì´ í•„ìš”í•¨\n",
    "        \n",
    "    print('\\n4. ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê¸°ë°˜ ì´ìƒê°’ íƒì§€')\n",
    "    business_outliers = (\n",
    "        (df['age'] >130) |\n",
    "        (df['days_since_last_purchase'] < 0) |\n",
    "        (df['avg_order_value'] > 1000000) |\n",
    "        (df['avg_order_value'] < 0)\n",
    "    )\n",
    "    print(f'  ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì´ìƒê°’: {business_outliers.sum()}ê°œ ({business_outliers.mean() * 100:.1f}%)')\n",
    "    \n",
    "    # ì¢…í•© íŒì •\n",
    "    outlier_summary = pd.DataFrame({\n",
    "        'IQR': univariate_outliers.sum(axis=1) > 0,\n",
    "        'ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬': mahalanobis_outliers,\n",
    "        'Isolation Forest': iso_outliers,\n",
    "        'ë¹„ì¦ˆë‹ˆìŠ¤': business_outliers,\n",
    "    })\n",
    "    \n",
    "    outlier_summary['ì´ì´ìƒê°’ìˆ˜'] = outlier_summary.sum(axis=1)\n",
    "    outlier_final = outlier_summary['ì´ì´ìƒê°’ìˆ˜'] >= final_threshold\n",
    "    print(f'\\n == ìµœì¢… ì´ìƒê°’: {outlier_final.sum()}ê°œ ({outlier_final.mean() * 100:.1f}%)')\n",
    "    \n",
    "    return outlier_summary\n",
    "    \n",
    "outlier_detection(df_knn, chi_q=0.999, iso_contamination='auto', final_threshold=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
