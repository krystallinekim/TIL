{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35420327",
   "metadata": {},
   "source": [
    "# 메모리\n",
    "\n",
    "대화 내용을 기억하는 것 - *요즘은 랭그래프에 다 흡수됨*\n",
    "\n",
    "- LLM은 기본적으로 대화 내용을 기억하지 않음(stateless) - input 1번에 output 1번\n",
    "\n",
    "- 계속 이전 대화내용을 프롬프트에 넣어줘야함\n",
    "\n",
    "1. Short-Term Memory(단기기억)\n",
    "\n",
    "    한 대화 세션 안에 대한 기억\n",
    "\n",
    "2. Long-Term Memory(장기기억)\n",
    "\n",
    "    전체 세션에서 추출한 중요한 정보 - GPT에서 메모리 업데이트됨 같은것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5144f10",
   "metadata": {},
   "source": [
    "## 메모리는 어디에 저장해야 하는가?\n",
    "\n",
    "1. 디스크\n",
    "\n",
    "    속도가 느림 / 저장하기 좋다(안정성이 높음)\n",
    "\n",
    "    기본적인 모든 데이터 저장\n",
    "\n",
    "2. 램\n",
    "\n",
    "    엄청 빠름 / 컴퓨터가 꺼졌다 켜지면 사라짐(휘발성)\n",
    "\n",
    "    자주 확인하는 데이터, 지금 사용하는 데이터들은 램에 저장 => **캐싱**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c7d1d",
   "metadata": {},
   "source": [
    "## Conversation Buffer Memory\n",
    "\n",
    "메시지를 저장한 뒤 변수에서 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "291e8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano', temperature=0)  # 사전적 의미: 랜덤성, 확률이 낮은 단어도 선택 가능함\n",
    "# temperature 높음: 설명을 하기 위해 다양한 언어를 사용, 지루하고 현학적임 - 창의적, 예술적이거나 생각이 필요한 경우는 temp가 높아야함\n",
    "# temperature 낮음: 팩트 위주, 딱 사실만 반영 - 논문 참조, 정보 확인 등 정확성이 중요할 경우 temp가 낮은게 맞음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fd5a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        ('system', '넌 정확한 정보를 전달하는 챗봇이야'),\n",
    "        MessagesPlaceholder(variable_name='chat_history'),  # 여기에 기존 채팅의 모든 내역을 주입함 - 내역을 넣을 공간을 미리 채워두는것\n",
    "        ('human', '{input}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history')\n",
    "\n",
    "# 메모리는 {}에 저장, 기존에 대화내용이 있다면 불러오기\n",
    "memory.load_memory_variables({})\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b91dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# chat_history에 load_memory_var 결과를 저장 후 키를 추출\n",
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | \n",
    "    itemgetter('chat_history')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d270a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='안녕', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='안녕하세요! 어떻게 도와드릴까요?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = runnable | prompt | llm\n",
    "\n",
    "my_message = '안녕'\n",
    "res = chain.invoke({'input': my_message})\n",
    "\n",
    "memory.save_context(\n",
    "    {'human': my_message},\n",
    "    {'ai': res.content}    \n",
    ")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788872e0",
   "metadata": {},
   "source": [
    "## 메모리 구동 방식\n",
    "기본적으로 대화 내용은 전부 프롬프트에 집어넣음\n",
    "\n",
    "대화가 길어지면 길어질수록, 소비하는 토큰 숫자가 늘어난다 + 뒤로 갈수록 성능도 낮아짐\n",
    "\n",
    "- 대화 요약 생성 후 요약을 저장(Conversation Summary Memory)\n",
    "- 최근 K개의 상호작용만 사용함(Conversation Buffer Window Memory)\n",
    "- 토큰 길이를 이용해 대화내용을 플러시할 시기를 결정(Conversation Token Buffer Memory)\n",
    "- ...\n",
    "\n",
    "이젠 볼 일이 없다. 전부 랭그래프로 기능이 넘어감."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a51b79d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나:\n",
      "안녕? 너가 메모리를 기억하는지 실험 중이야. 뭔가 기억하고 싶은 게 있니?\n",
      "AI:\n",
      "안녕하세요! 저는 지금 현재 대화 세션 내에서만 정보를 기억할 수 있어서, 이전 만남이나 내용을 계속 기억하지는 못합니다. 그렇지만, 만약 기억했으면 하는 중요한 내용이 있다면 말씀해 주시면 계속 이어서 도움을 드릴 수 있습니다. 어떤 내용을 기억했으면 좋겠나요?\n",
      "---\n",
      "나:\n",
      "너한테 물어보잖아. 뭔가 기억하고 싶은게 있냐고\n",
      "AI:\n",
      "네, 질문해 주셔서 감사합니다. 저는 인공지능이기 때문에 개인적인 기억이나 바람이 있는 것은 아니지만, 만약 제가 기억한다면, 사용자님과의 대화에서 유용하거나 의미 있다고 느낄 만한 중요한 정보들을 기억하고 싶어요. 예를 들어, 사용자님이 특별히 관심 있는 분야나 필요한 정보 등을 기억한다면 더 잘 도와드릴 수 있을 것 같습니다. 혹시 어떠한 정보나 주제를 기억했으면 좋겠다고 생각하시는 게 있으신가요?\n",
      "---\n",
      "나:\n",
      "그럼 내가 무채색을 좋아한다는 사실을 기억해봐\n",
      "AI:\n",
      "알겠습니다. 사용자님께서 무채색을 좋아하신다는 사실을 기억하겠습니다. 앞으로 대화할 때 참고할게요. 혹시 더 나누고 싶은 정보가 있으시면 언제든 말씀해 주세요.\n",
      "---\n",
      "나:\n",
      "내가 어떤 색을 좋아한다 그랬지?\n",
      "AI:\n",
      "네, 사용자님께서 무채색을 좋아하신다고 말씀하셨죠. 앞으로 대화에서 그 점을 참고하겠습니다. 더 궁금하신 점이나 나누고 싶은 이야기가 있으시면 언제든 말씀해 주세요.\n",
      "---\n",
      "나:\n",
      "그만\n",
      "AI:\n",
      "알겠습니다. 언제든 필요하시면 말씀해 주세요. 좋은 하루 되시길 바랍니다.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# 실제 gpt랑 대화하는 식으로 만들어 보기\n",
    "from dotenv import load_dotenv\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "input_msg = ''\n",
    "\n",
    "# 메모리\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history')\n",
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | \n",
    "    itemgetter('chat_history')\n",
    ")\n",
    "\n",
    "# 프롬프트\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        ('system', '항상 존댓말로 대답해'),\n",
    "        MessagesPlaceholder(variable_name='chat_history'),\n",
    "        ('human', '{input}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano', temperature=1)\n",
    "\n",
    "# Chain\n",
    "chain = runnable | prompt | llm | StrOutputParser()\n",
    "\n",
    "# 정지 신호 설정\n",
    "while input_msg not in ('멈춰', '그만', '정지'):\n",
    "    input_msg = input()\n",
    "    # 대화 로직 + 대화내역 출력, 정지신호 시 대화 종료\n",
    "    print('나:')\n",
    "    print(input_msg)\n",
    "    print('AI:')\n",
    "    \n",
    "    # 대답 생성\n",
    "    ai_response = ''\n",
    "    for token in chain.stream({'input': input_msg}):\n",
    "        print(token, end='', flush=True)\n",
    "        ai_response += token\n",
    "    \n",
    "    # 메모리에 응답 저장\n",
    "    memory.save_context(\n",
    "        {'human': input_msg},\n",
    "        {'ai': ai_response}    \n",
    "    )\n",
    "    \n",
    "    print('\\n---')\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "memory.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
