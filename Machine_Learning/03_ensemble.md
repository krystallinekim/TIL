# 앙상블

여러 개의 약한 개별 모델을 조합(ensemble)해서 하나의 예측 모델을 만드는 방법

정형 데이터 (Structured Data) 기준으로는 가장 뛰어난 성과를 내는 알고리즘

대부분의 앙상블 학습은 트리 기반이다

## 트리 구조

그래프 = 점(node)과 점 사이의 연결선(edge)을 포함한 자료 구조 -> 연결된 노드 간의 관계를 표현할 수 있다. 

트리 = 어디에서도 순회하지 않는 그래프 -> 특정 노드를 루트 노드로 정하면, 모든 노드를 부모-자식 관계로 표현할 수 있다.

모든 트리에는 루트 노드 1개, 수많은 리프 노드가 있다.

## Random Forest - 랜덤 포레스트

### Decision Tree - 결정 트리(분류)

결정 트리는 트리 구조로 전체 데이터에 대해 스무고개를 돌려서 전체 데이터를 분류하는 게 목적인 분석 방법

```py
from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier(max_depth=5, random_state=42)
dtc.fit(X_train, y_train)
```
fit을 돌리면, 데이터에 특정 기준을 적용하고, 그렇게 분류된 데이터에 대해 불순률을 계산하고, 아직 불순률이 있다면 다른 기준을 적용해 또 나누는 작업을 반복하게 된다.

이 때, 머신러닝은 이런 기준들을 계속 랜덤하게 적용하게 됨

결정트리 분류에서 `hyperparameter`는 `max_depth`이다. 최대 depth의 크기로, 루트에서 몇단계까지 내려가는 것 까지 볼 수 있는지 정해주는 것

**불순률**

일반적으로, gini 불순도를 사용해 분포가 섞여 있음을 나타내게 된다.

$$
Gini(t) = 1 - \sum_{i=1}^{C} p_i^2
$$
- $C$ : 클래스 개수
- $p_i$ : 노드 $t$ 에서 클래스 $i$ 가 차지하는 비율

이진 분류일 경우, C=2이므로

$$
Gini(t) = 1 - (p_0^2 + p_1^2)
$$

처럼 만들 수 있다.

- gini = 0.5 : 비율이 1:1, 하나도 분류가 안된것
- gini ≈ 0   : 비율이 한쪽으로 몰림. 분류가 꽤 잘 된 것
- gini = 0   : 클래스 하나만 남아서 분류가 끝난 것

### Random Forest

결정 트리 구조를 여러개 돌려서 랜덤 '포레스트' 분석

1. 결정트리를 랜덤하게 만들어 트리의 숲을 만듦

2. 각 결정트리의 예측을 종합해 최종 예측을 만듦

- 과대적합(overfitting)에 안전!

데이터가 1000개면, 각 트리마다 1000개 데이터를 복원추출함. 이때 중복을 허용 (우연히 같은데이터만 1000개 가능)

노드 분할시, 분류 / 회귀의 특성 선택방식이 다름(분류: 개수를 루트함, 회귀: 특성을 다 씀)

기본값 100개의 트리를 만들어서

- 분류: 다수결 투표

- 회귀: 100개의 평균



## Extra Tree

## Gradient Boosting